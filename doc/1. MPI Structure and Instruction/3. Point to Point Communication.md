# 3. Point-to-Point Communication
- Author: JaeDuk Yoo(woejr120@gmail.com)
- Date: 2019.06.28
---
***
## Point-to-Point Communication
- 랭크와 랭크가 1 대 1로 통신하는 방식을 의미한다.


### Communication Routine
- 송신 루틴은 총 4종류가 존재(단, 블로킹 모드와 논블로킹 모드가 각각 존재한다.)
  - **동기 송신**: 안전하지만 비효율적
    1. 송신 프로세스(S)가 수신 프로세스(D)에 송신 준비가 완료됐다고 알림
    2. 수신 프로세스(D)는 수신 루틴을 수행한 후 송신 프로세스에 수신 준비가 완료됐다고 알림
    3. 송신 프로세스(S)가 수신 준비 완료 메시지를 받으면 데이터 전송을 시작한다.

    ![Synchronous Send](http://k-atoms.ksc.re.kr/mpi/images/2_2_1_2.jpg)

  - **준비 송신**: 성능 향샹에는 유리하지만 메시지를 제대로 전달받지 못하는 상황이 생기며 디버깅이 어려움
    - 수신 프로세스(D)가 수신 준비 완료 상태라고 가정하고 수행한다.
    - 송신 프로세스(S)가 송신 준비를 완료하자마자 데이터 전송을 시작
    - 만약 수신 프로세스(D)가 수신 루틴을 수행하지 못한 상태라면, 해당 메시지는 사라지고 오류가 발생한다.

    ![Ready Send](http://k-atoms.ksc.re.kr/mpi/images/2_2_1_3.jpg)

  - **버퍼 송신**: 버퍼를 위한 추가적인 메모리가 요구되지만, 메시지를 전달받지 못하는 상황은 생기지 않는다.
    - 버퍼 송신 루틴이 호출되면 사용자가 정의한 버퍼로 데이터를 저장한다.
    - 이후 수신 루틴이 호출되면 수신 프로세스는 버퍼에 저장된 데이터를 가져온다.
    - 사용자는 MPI 루틴 MPI_Buffer_attach와 MPI_Buffer_detach를 이용해 통신에 이용할 버퍼를 확보하고 해제 시킬 수 있다.

    ![Buffer Send](http://k-atoms.ksc.re.kr/mpi/images/2_2_1_4.jpg)

  - **표준 송신**: 상황에 따른 안정성과 성능을 고려하여 두가지 모드로 분할
    - 송신 데이터 크기가 임계점보다 작으면 버퍼 송신을 통해 데이터 송수신
      - 데이터의 크기가 작을 경우, 비효율적인 동기 통신 대신 버퍼 통신을 사용
      - 이때, 표준 모드에서 사용되는 버퍼는 프로그램 시작과 동시에 프로세스마다 하나씩 자동으로 부가해주기 때문에 사용자의 관리가 필요 없다.

    ![Standard Send A](http://k-atoms.ksc.re.kr/mpi/images/2_2_1_5.jpg)

    - 송신 데이터 크기가 임계점보다 크면 동기 송신을 통해 데이터 송수신
      - 데이터의 크기가 클 경우, 버퍼에 모두 들어가지 않을 수 있음 -> 안정성을 위해 동기 통신 사용
      - 임계값을 0으로 설정하면 모든 통신을 동기 통신으로 수행한다.

    ![Standard Send B](http://k-atoms.ksc.re.kr/mpi/images/2_2_1_6.jpg)


- 수신 루틴은 총 2종류 존재
  1. 블로킹 수신(MPI_REV): 수신받을동안 프로세스가 다른 작업을 수행할 수 없음.
  2. 논블로킹 수신(MPI_ISEND): 수신받을동안 프로세스가 다른 작업을 병행한다.


![Point to Point Communication](http://k-atoms.ksc.re.kr/mpi/images/2_2_1_1.jpg)

---

### Blocking Standard Send
- Blocking: 함수 내에서 통신 모듈 초기화와 종료가 모두 실행되기 때문에 통신 루틴을 호출한 두 프로세스가 통신이 완료될 때까지 다른 작업을 못하도록 막는 것.
#### Function
1. Blocking Send
  ```c
  int MPI_Send(void *data, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)
  // blocking send
  // data: address of data, count: size, dest: destination rank, tag: message tag , comm: communication
  ```
   - datatype 타입의 count개 원소들을 buf 에서 dest 프로세스로 송신한다. dest는 커뮤니케이터 comm에 속한 총 n개의 프로세스중에 하나의 프로세스 랭크를 나타낸다. 송신된 메시지는 MPI_RECV 나 MPI_IRECV 로 수신할 수 있다.

2. Blocking Receive
  ```c
  int MPI_Recv(void *data, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)
  // blocking receive
  // data: address of data, count: size, source: source rank, tag: message tag, comm: community
  ```
   - 인수 status 는 수신된 메시지에 대한 source 와 꼬리표 등의 정보를 출력하는 MPI_STATUS_SIZE 원소들을 가지는 정수형 배열이다.
   - 만약 수신 버퍼(buf)의 크기가 데이터의 크기보다 크다면 오버플로우 오류가 발생한다.
     - 수신 버퍼와 버퍼 통신을 헷갈리지 말 것.
   - Wild Card: 모든 프로세서로부터의 모든 메시지를 수신한다.
     - source -> MPI_ANY_SOURCE, tag -> MPI_ANY_TAG
   - mpi_status: 현재 통신에 관련된 정보를 저장해둠.

     ![mpi_status](http://k-atoms.ksc.re.kr/mpi/images/2_2_2_1.jpg)

3. MPI_Get_count
    ```c 
    int MPI_Get_count(MPI_Status *status, MPI_Datatype datatype, int *count)
    // status: message를 특정할 때 활용
    // count: size
    ```
#### 예제
```c
#include <stdio.h>
#include <mpi.h>

void main(int argc, char *argv[]) {
  int rank, i, count;

  float data[100],value[200];

  MPI_Status status;

  MPI_Init(&argc,&argv);
  // initialize MPI

  MPI_Comm_rank(MPI_COMM_WORLD,&rank);
  // seperate rank

  if(rank==1) {

  for(i=0;i<100;++i) data[i]=i;
    MPI_Send(data,100,MPI_FLOAT,0,55,MPI_Comm_world);
    // buffer -> data, count -> 100, datatype -> MPI_FLOAT, dst -> rank 0, tag -> 55, communicator -> MPI_Comm_world
    // if rank is 1, send data to rank 0
  }

  else {
    MPI_Recv(value,200,MPI_FLOAT,MPI_ANY_SOURCE,55, MPI_COMM_WORLD,&status);
    // buffer -> value, count -> 200, datatype -> MPI_FLOAT, source -> all, tag -> all, commuicator -> all
    // if rank is not 1, receive data from all the other ranks

    printf("P:%d Got data from processor %d \n",rank, status.MPI_SOURCE);
    // rank: current rank, status.MPI_SOURCE: 1

    MPI_Get_count(&status,MPI_FLOAT,&count);
    // status -> status, datatype -> MPI_FLOAT, count -> count
    printf("P:%d Got %d elements \n",rank,count);
    printf("P:%d value[5]=%f \n",rank,value[5]);
  }
  MPI_Finalize();
}
```

- 실행 결과
  1. rank 1
     - data: 전송할 데이터가 들어가있음. 해당 데이터는 다른 프로세스에서 해당 송수신을 수행할 때마다 참조된다.
     - value: 대입된 값이 하나도 없으므로 쓰레기값만 들어가있음
  2. else
     - data: 대입한 값이 하나도 없으므로 쓰레기값만 들어가있음
     - value: process A로부터 전송받은 데이터를 해당 위치에 저장.
- data와 value 값들은 모든 노드마다 **개별적으로** 존재한다. -> rank 0, 1, 2의 data는 동일한 이름이라고 해도 서로 다른 메모리의 다른 주소에 존재할 가능성이 높다.
    ```text
        memory of Rank 0     memory of Rank 1
    0x0000000[    ]              [    ] <- Data
    0x0000004[    ] <- Data      [    ] 
    .                            .
    .                            .
    .                            .
    0x5A38124[    ]              [    ]
    0x5A38128[    ]              [    ] <- Value
    0x5A3812C[    ] <- Value     [    ] 
    0x5A38130[    ]              [    ]
    0x5A38134[    ]              [    ]
    0x5A38138[    ]              [    ]
    ```

---
- 의문점
  1. 버퍼 통신에서의 버퍼와 명령어에서의 버퍼는 서로 동일한 것을 의미하는가?
  2. 버퍼 통신 방식으로 Rank 0에서 Rank 1로 특정 데이터를 전송할 때, 와일드 카드를 쓰면 다른 랭크에서도 이를 가져올 수 있나?
---

#### Deadlock(교착)
- Deadlock: 2개의 프로세스가 각각 블로킹되어 각각 다른 프로세스의 진행을 기다리는 상황
  - 데이터 수신 및 발신할 동안 프로세스를 진행하지 않은 블로킹 전송의 특성상, 서로 막혀 프로그램이 진행되지 않는다.

##### 예제
- **deadlock example A**
  ```c
  #include <mpi.h>

  #include <stdio.h>

  void main (int argc, char *argv[]){

    int nprocs, myrank ;

    MPI_Status status;

    double a[100], b[100];

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    if (myrank ==0) {
      MPI_Recv(b, 100, MPI_DOUBLE, 1, 19, MPI_COMM_WORLD, &status);
      // waiting for data from rank 1
      MPI_Send(a, 100, MPI_DOUBLE, 1, 17, MPI_COMM_WORLD);
    }


    else if (myrank==1) {
      MPI_Recv(b, 100, MPI_DOUBLE, 0, 17, MPI_COMM_WORLD, &status);
      // waiting for data from rank 0
      MPI_Send(a, 100, MPI_DOUBLE, 0, 19, MPI_COMM_WORLD);
    }

    MPI_Finalize();

  }
  ```
  - 0번 프로세스와 1번 프로세스가 서로 데이터를 전송하기를 기다리고 있음 -> 블로킹의 특성상 데이터 송수신이 완료될 때까지 프로그램을 진행하지 않는다 -> 둘다 데이터를 보내지 않으므로 프로그램이 더 이상 진행되지 않음

- **deadlock example B**
  ```c
  #include <mpi.h>

  #include <stdio.h>

  void main (int argc, char *argv[]){

    int nprocs, myrank ;

    MPI_Status status;

    double a[100], b[100];

    MPI_Init(&argc, &argv);

    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);

    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    if (myrank ==0) {
      MPI_Send(a, 100, MPI_DOUBLE, 1, 17, MPI_COMM_WORLD);
      // send first
      MPI_Recv(b, 100, MPI_DOUBLE, 1, 19, MPI_COMM_WORLD, &status);
    }


    else if (myrank==1) {
     MPI_Send(a, 100, MPI_DOUBLE, 0, 19, MPI_COMM_WORLD);
     // send first
     MPI_Recv(b, 100, MPI_DOUBLE, 0, 17, MPI_COMM_WORLD, &status);
    }

    MPI_Finalize();
  }
  ```
  - 만약 데이터의 크기가 작아 MPI_Send가 버퍼 송신으로 작동한다면 문제 없다.
  - 단, 데이터의 크기가 커지고 MPI_Send가 동기화 송신으로 작동한다면 rank 0는 rank 1이 수신 준비가 되기를, rank 1은 rank 0가 수신준비가 되기를 기다리게되므로 교착 상태에 빠진다.

- **solution: exchange**
  ```c
  #include <mpi.h>

  #include <stdio.h>

  void main (int argc, char *argv[]){

    int nprocs, myrank ;

    MPI_Status status;

    double a[100], b[100];

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    if (myrank ==0) {
      MPI_Send(a, 100, MPI_DOUBLE, 1, 17, MPI_COMM_WORLD);
      MPI_Recv(b, 100, MPI_DOUBLE, 1, 19, MPI_COMM_WORLD, &status);
      // waiting for data from rank 1
    }


    else if (myrank==1) {
      MPI_Recv(b, 100, MPI_DOUBLE, 0, 17, MPI_COMM_WORLD, &status);
      // waiting for data from rank 0
      MPI_Send(a, 100, MPI_DOUBLE, 0, 19, MPI_COMM_WORLD);
    }

    MPI_Finalize();

  }
  ```
  - rank 0 -> rank 1 전송이 먼저 수행되고 그 이후 rank 1 -> rank 0 전송을 수행.

***
### Non-Blocking Standard Send
- 블로킹 송수신은 송수신이 완료될때까지 프로그램을 정지하므로 프로그램 자체가 느려지고 교착(deadlock) 상태가 될 수 있다. -> 이 문제를 해결하기 위해 논블로킹 송수신이 만들어짐
- Non-Blocking Communication: 송수신 연산의 초기화와 종료를 서로 분리해 두 프로그램 사이에 다른 작업을 수행할 수 있도록 만듦.

#### Communication Routine
- Non-Blocking Routine
1. MPI 통신 초기화 루틴 실행(Posting)
2. 초기화 완료 후 함수 종료
3. 이후 통신은 백그라운드 환경에서 계속되며 사용자가 필요할때만 결과를 확인하거나 통신을 기다리게 할 수 있다.
- 데이터의 크기가 버퍼보다 작을 경우 -> 동일하게 버퍼 통신

  ![Non-Blocking A](http://k-atoms.ksc.re.kr/mpi/images/2_2_3_1.jpg)

  - 만약 수신 버퍼에 있는 데이터를 바로 사용해야할 경우, MPI_Wait 명령어를 사용해 송수신이 끝날때까지 대기한다.
- 데이터 크기가 버퍼보다 클 경우 -> 동기화 통신과 유사하게 작동한다.

  ![Non-Blocing B](http://k-atoms.ksc.re.kr/mpi/images/2_2_3_2.jpg)

  - 만약 동기화 수신 루틴이 미리 호출된 상태라면 더 빠르게 수행 가능함.


#### Function
1. Non-Blocking Send
  ```c
  int MPI_Isend(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request *request)
  // buf: buffer(Sending Data), count: data num, datatype: datatype, dest: destination, tag: tag,  comm: communication
  // request: seperating init request and sending request
  ```
  - Request를 통해 통신을 식별한다. -> status와 비슷한 역할이라 보면 됨

2. Non-Blocking Receive
  ```c
  int MPI_Irecv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Request *request)
  // buf: buffer(Receiving Pointer), count: data size, datatype: datatype, dest: destination, tag: tag, comm: communication
  ```
  - 해당 함수는 시스템이 수신 버퍼에 데이터를 받아올 준비가 끝났을때만 호출된다.
  - 논 블로킹 수신이 호출된 후, 해당 수신이 끝날때까지 수신 버퍼에 접근할 수 없으며(바로 호출해야할 경우 wait함수 사용), 수신된 메시지의 크기가 수신 버퍼보다 작으면 오버플로우 오류 발생.
  - 블로킹 송신과 논블로킹 송신 둘다 호환된다.

3. Wait
  ```c
  int MPI_Wait(MPI_Request *request, MPI_Status *status)
  // request: identifing communcation, status: return current status 
  ```
  - request를 통해 통신을 식별한 후, 통신이 완료될때까지 프로세스를 멈춘다.
  - 논블로킹 연산 후 바로 MPI_Wait함수를 사용할 경우, 블로킹 함수와 차이가 없음.

4. Test
  ```c
  int MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)
  ```
  - request를 통해 연산을 식별한 후, 연산의 완료 여부를 검사하여 인스 flag를 통해 즉시 리턴한다.

#### 예제
- 논블로킹 표준 통신을 이용한 교착 피하기

  ```c
  /*avoid_deadlock*/

  #include <mpi.h>
  #include <stdio.h>

  void main (int argc, char *argv[]){
    int nprocs, myrank ;
    MPI_Request request;
    MPI_Status status;
    double a[100], b[100];

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    if (myrank ==0) {
      MPI_Irecv(b, 100, MPI_DOUBLE, 1, 19, MPI_COMM_WORLD, &request);
      MPI_Send(a, 100, MPI_DOUBLE, 1, 17, MPI_COMM_WORLD);
      MPI_Wait(&request, &status);
    }

    else if (myrank==1) {
      MPI_Irecv(b, 100, MPI_DOUBLE, 0, 17, MPI_COMM_WORLD, &request);
      MPI_Send(a, 100, MPI_DOUBLE, 0, 19, MPI_COMM_WORLD);
      MPI_Wait(&request, &status);
    }

    MPI_Finalize();
  }
  ```

### Problem
1. 1부터 100까지의 숫자를 n개의 랭크로 나누어 연산하는 코드를 작성하시오.
  
    ![result](img/ch1_example2.png)

   - 결과값이 달라지는 이유에 대해 생각해보자

2. 점대점 통신을 사용해 2개의 노드가 번갈아가며 1부터 10까지 1씩 증가된 숫자를 출력하는 코드를 쓰시오.

    ![result](img/ch1_example1.png)

---