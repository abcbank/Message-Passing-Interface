---
title: 4. Collective Communication
author: JaeDuk Yoo (woejr120@gmail.com)  
date: 2019-07-11
---
# 4. Collective Communication
---
---
## Collective Communication

### 방송(Broadcast)
- 방송: 루트 프로세스(rank 0)의 메모리를 동일한 커뮤니케이터 내의 모든 프로세스들의 메모리의 같은 위치로 복사하는 일. 루트 하나와 그 외 모든 프로세스가 통신하므로 일대다 통신에 해당된다.
    ![BroadCast](http://k-atoms.ksc.re.kr/mpi/images/2_3_1_1.jpg)

#### Function
```c
int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)
```

#### 예제
```c
/*broadcast*/

#include <mpi.h>
#include <stdio.h>

void main (int argc, char *argv[]){
    int i, myrank;
    int imsg[4];
    int jmsg[4];

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    if (myrank==0) for(i=0; i<4; i++) 
        imsg[i] = i+1;
    else for (i=0; i<4; i++) 
        jmsg[i] = imsg[i] = 0;
    
    #ifdef SAME_BUF
    printf("MPI with same buffer\n");
    printf( "%d: BEFORE:" , myrank);
    for(i=0; i<4; i++) 
        printf( " %d" , imsg[i]);
    printf( "\n" );

    MPI_Bcast(imsg, 4, MPI_INTEGER, 0, MPI_COMM_WORLD);

    printf( "%d: AFTER:" , myrank);

    for(i=0; i<4; i++) 
        printf( " %d" , imsg[i]); 
    printf( "\n" );
    #endif

    #ifndef SAME_BUF
    if(myrank == 0)
        MPI_Bcast(imsg, 4, MPI_INTEGER, 0, MPI_COMM_WORLD);
    else
        MPI_Bcast(jmsg, 4, MPI_INTEGER, 0, MPI_COMM_WORLD);

    #endif

    MPI_Finalize();
}
```
- 첫번쨰 경우에선 같은 변수(버퍼)를 루트에선 수신 버퍼, 루트 외의 프로세스에선 송신 버퍼으로 사용하고 있다.
- 두변째 경우는 수신 버퍼와 송신 버퍼를 서로 나누고 싶을때 사용한다.

***
### 취합(Gather)
- **MPI_Gather**
  - 커뮤니케이터에 있는 모든 프로세스들이 하나의 수신 프로세스로 데이터를 전송하는 다대일 통신. 방송의 반대 개념이라고 보면 됨.
  - 해당 루틴이 호출될 경우, 수신 프로세스를 포함한 모든 프로세스들은 송신 버퍼에 있는 데이터를 수신 프로세스에 전송하고, 수신 프로세스는 이를 받아 랭크 순서대로 메모리에 저장한다.
  - 해당 함수에서 전송하는 데이터의 크기는 모두 같아야한다.
  - 수신 프로세스 또한 데이터를 송신하는 개념이므로, 수신 프로세스 내의 수신 버퍼의 위치와 송신 버퍼의 위치는 서로 다르다.
      ![gather](http://k-atoms.ksc.re.kr/mpi/images/2_3_2_1.jpg)
        - isend: sending buffer, irecv: receiving buffer name

- **MPI_Gatherv**
  - 프로세스마다 서로 다른 크기의 데이터를 송신 가능
  - 송신 프로세스는 서로 다른 세 개의 포인터를 사용한다. -> 이중 포인터와 비슷하다고 생각하면 됨
    - displs: 취합한 자료 내에서 각각 랭크가 시작하는 위치를 특정해줌
    - recvcounts: 각각 랭크마다 보낸 자료 수를 저장
    - recvbuf: 랭크에 대한 자료들을 일렬로 저장
  - 이후 추가적인 예제를 실행해보고, 이중포인터와 호환성을 점검해볼 것.
    ![gatherv](http://k-atoms.ksc.re.kr/mpi/images/2_3_2_2.jpg)

- **MPI_Allgather**
  - 쉽게 말하면 취합과 방송을 한번에 하는 명령어
  - 데이터를 취합한 후, 해당 데이터를 방송한다.
    ![Allgather](http://k-atoms.ksc.re.kr/mpi/images/2_3_2_3.jpg)

- **MPI_Allgatherv**
  - 서로 크기가 다른 데이터를 주고받을때 사용
  - 저장 알고리즘은 MPI_Gatherv와 동일하며, 이후 저장된 데이터를 방송한다.
    ![Allgatherv](http://k-atoms.ksc.re.kr/mpi/images/2_3_2_4.jpg)

#### Function
- Gather
    ```c
    int MPI_Gather(void *sendbuf, int sendcount, MPI_Datatype datatype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)
    int MPI_Allgather(void *sendbuf, int sendcount, MPI_Datatype datatype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)
    // buf: data, count: size, type: datatype, root: receiving process rank
    ```
    - sendcount는 고정되어있음

- MPI_Gatherv
    ```c
    int MPI_Gatherv(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int displs, MPI_Datatype recvtype, int root, MPI_Comm comm)
    int MPI_Gatherv(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int *displs, MPI_Datatype recvtype, int root, MPI_Comm comm)
    ```
    - sendcount가 유동적임
    - 이때, displs는 input이라는 점을 명심하자.
      - displs의 작용에 대해선 더 실험해봐야함. 해당 값을 사용자가 정하는 것인지, 혹은 데이터의 크기만큼 대입이 되는건지 확인해볼것.

#### 예제
- MPI_Gather
    ```c
    /*gather*/
    #include <mpi.h>
    #include <stdio.h>
    void main (int argc, char *argv[]){
        int i, nprocs, myrank ;
        int isend, irecv[3];
        MPI_Init(&argc, &argv);
        
        MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
        MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
        
        isend = myrank + 1;
        
        MPI_Gather(&isend,1,MPI_INTEGER,irecv,1,MPI_INTEGER,0, MPI_COMM_WORLD);
        
        if(myrank == 0) {
            printf( " irecv = " );
            for(i=0; i<3; i++) printf( " %d" , irecv[i]); printf( "\n" );
        }

    MPI_Finalize();
    }
    ```
    - 이때, 수신 데이터의 총 크기는 송신 데이터의 크기와 커뮤니케이터의 총 랭크의 개수를 곱해준 값이라는 것을 유의하자.

- MPI_Gatherv
    ```c
    /*gatherv*/

    #include <mpi.h>
    #include <stdio.h>

    void main (int argc, char *argv[]){
        int i, myrank ;
        int isend[3], irecv[6];
        int iscnt, ircnt[3]={1,2,3}, idisp[3]={0,1,3};

        MPI_Init(&argc, &argv);
        MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

        for(i=0; i<myrank+1; i++) 
            isend[i] = myrank + 1;
        
        iscnt = myrank +1;
        MPI_Gatherv(isend, iscnt, MPI_INTEGER, irecv, ircnt, idisp, MPI_INTEGER, 0, MPI_COMM_WORLD);
        
        if(myrank == 0) {
            printf( " irecv = " );
            for(i=0; i<6; i++) 
                printf( " %d" , irecv[i]);
            printf( "\n" );
        }
        
        MPI_Finalize();
    }
    ```

- MPI_Allgather
    ```c
    /*allgather*/
    #include <mpi.h>
    #include <stdio.h>
    void main (int argc, char *argv[]){
        int i, myrank ;
        int isend, irecv[3];
        
        MPI_Init(&argc, &argv);
        MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
        
        isend = myrank + 1;
        
        MPI_Allgather(&isend, 1, MPI_INTEGER, irecv, 1, MPI_INTEGER, MPI_COMM_WORLD);
        
        printf( "%d irecv = " );
        for(i=0; i<3; i++) 
            printf( " %d" , irecv[i]);
        printf( "\n" );

        MPI_Finalize();
    }
    ```
***
### 환산(Reduce)
- MPI_Reduce
  - 각 프로세스로부터 데이터를 모아 하나의 값으로 연산한 후 그 결과를 수신 프로세스(루트)에 저장한다.
    1. 스칼라 값의 전송
      ![reduce1](http://k-atoms.ksc.re.kr/mpi/images/2_3_3_1.jpg)
    2. 배열의 전송
      ![reduce2](http://k-atoms.ksc.re.kr/mpi/images/2_3_3_5.jpg)
  - rounding error: 일반적으로 정해진 정밀도의 실수를 연산할때, (a+b)+c와 a+(b+c)는 서로 다른 결과를 갖는다.->일정 이하의 소수점은 버려서 그런듯
    - 따라서 덧셈이나 뺄셈 또한 연산 순서에 미세하게 영향받는다는 사실을 유의하도록 하자.

- MPI_Allreduce
  - Allgather과 마찬가지로 연산 결과를 모든 프로세스의 수신 버퍼로 전송한다.
    ![allgather](http://k-atoms.ksc.re.kr/mpi/images/2_3_3_6.jpg)
    - 모든 프로세스의 수신 버퍼의 프로세스에 할당된 연산결과를 전송한다고 이해하는 것이 더 편하다.
#### Function
```c
int MPI_Reduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)
int MPI_Allreduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)
// op: operator
```
- 매개변수 op를 통해 데이터를 어떤 연산을 통해 불러올지 결정한다.
  - op자리에 선언되는 변수들은 MPI 헤더파일에 정의되어있으며, MPI_OP_CREATE를 통해 자신이 원하는 연산을 새로 선언할 수도 있다.
    ![operator](http://k-atoms.ksc.re.kr/mpi/images/2_3_3_3.jpg)
#### 예제
- 스칼라 값을 전송할때
    ```c
    /*reduce*/
    #include <mpi.h>
    #include <stdio.h>
    void main (int argc, char *argv[]){
        int i, myrank, ista, iend;
        double a[9], sum, tmp;
        
        MPI_Init(&argc, &argv);
        MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
        
        ista = myrank*3;
        iend = ista + 2;
        
        for(i = ista; i<iend+1; i++) 
            a[i] = i+1;

        sum = 0.0;
        
        for(i = ista; i<iend+1; i++) 
            sum = sum + a[i];
        // 부분합 구하기. 해당 연산은 reduce를 통해 하는것이 아님을 유의하자.

        MPI_Reduce(&sum, &tmp, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
        // sum: sendbuf, tmp: recvbuf
        sum = tmp;
    
        if(myrank == 0) 
            printf("sum = %f \n" , sum);

        MPI_Finalize();
    }
    ```
    1. 각 프로세스 내에서 배열 초기화 및 부분합 연산 실행
    2. 연산 결과를 reduce를 통해 덧셈의 형태로 가져온다
    3. reduce의 결과를 원하는 변수에 대입한다.

- 배열을 전송할때
    ```c
    /*reduce*/
    #include <mpi.h>
    #include <stdio.h>
    void main (int argc, char *argv[]){
        int i, myrank, ista, iend;
        double a[9], sum, tmp[3];
        
        MPI_Init(&argc, &argv);
        MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
        
        ista = myrank*3;
        iend = ista + 2;
        
        for(i = 0; i < 3; i++)
            tmp[i] = 0.0;

        for(i = ista; i<iend+1; i++) 
            a[i] = i+1;

        sum = &a[ista];

        MPI_Reduce(sum, tmp, 3, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
        // sum: sendbuf, tmp: recvbuf
        sum = tmp[0] + tmp[1] + tmp[2];
    
        if(myrank == 0) 
            printf("sum = %f \n" , sum);

        MPI_Finalize();
    }
    ```
***
### 확산(Scatter)
1. MPI_Scatter
   - 데이터를 같은 크기로 나누어 커뮤니케이터의 프로세스에 랭크 순서대로 전송한다.
   - N개의 전송 프로토콜을 동시에 수행한다고 생각하면 됨.
    ![Scatter](http://k-atoms.ksc.re.kr/mpi/images/2_3_4_1.jpg)

2. MPI_Scatterv
   - 데이터를 임의의 크기로 나누어 커뮤니케이터의 프로세스에 랭크 순서대로 전송한다.
    ![Scatterv](http://k-atoms.ksc.re.kr/mpi/images/2_3_4_2.jpg)

#### Function
```c
int MPI_Scatter(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvdatatype, int root, MPI_Comm comm)
int MPI_Scatterv(void *sendbuf, int sendcount, int displs, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvdatatype, int root, MPI_Comm comm)
```
- sendcount: mpi_scatter에선 하나의 값을 가져야하지만 mpi_scatterv에선 서로 다른값을 갖는다.
- displs: 각 랭크에 보내는 데이터가 시작하는 배열의 시작 원소를 의미한다. -> gatherv 예제 보며 같이 이해할 것.

#### 예제
```c
/*scatter*/
#include <mpi.h>
#include <stdio.h>
void main (int argc, char *argv[]){
    int i, myrank ;
    int isend[3], irecv; 
    
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    for(i=0; i<nprocs; i++) 
        isend[i]=i+1;
    
    MPI_Scatter(isend, 1, MPI_INTEGER, &irecv, 1, MPI_INTEGER, 0, MPI_COMM_WORLD);
    
    printf( " %d: irecv = %d\n" , myrank, irecv);
    
    MPI_Finalize();
}
```
***