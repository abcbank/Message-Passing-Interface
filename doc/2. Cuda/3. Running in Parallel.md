# 2. Running in Parallel
---
---
## Running in Parallel

### Revisiting Device Function
```c
#include <iostream>

using namespace std;
__global__ void mykernel(void){

}
int main(void){
    mykernel<<<1,1>>>();

    printf("Hello World!\n");

    return 0;
}
```
- `__global__`: means runs on the device(device components). those function most called form host code(main thread)
- `mykernel<<<1,1>>>()`: calling device code
  - Also called a "kernel launch"
  - return to the parameters (1,1) -> to return to the parameters (N,1)?

### Vector Addtion
- `add<<<N,1>>>();`: execute add() N time in parallel
  - Grid Abstraction
    ```text

    [add(0,1)]-------> N개의 add 함수가 동시에 실행.
    [add(1,1)]   |
    .            |
    .            |
    .            |
    [add(N-1,1)]--
    ```
  - Grid의 세로 축 원소들(add(1,1), add(2,1) 등등)은 각각 블록(block)이라고 부른다.
  - 현재 블록의 인덱스(index: N of add(N,1))는 `blockIdx.x`에 저장되어 있다. -> 아래 예제를 보면서 이해할 것.
    
#### Vector Addition - Device Code
```c
__global__ void add(int *a, int *b, int *c){
    c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x];
}
```
- Grid Abstraction
    ```text
    Block 0: [c[0] = a[0] + b[0]] ---> vector의 크기는 main 함수에서 입력받는다.
    Block 1: [c[1] = a[1] + b[1]]
    Block 2: [c[2] = a[2] + b[2]]
    Block 3: [c[3] = a[3] + b[3]]
    .
    .
    .
    ```

#### Vector Addition - Host Code
```c
int main(void){
    int *a, *b, *c;
    int *d_a, *d_b, *d_c;
    int size = N * sizeof(int);
    // N -> size of vector

    cudaMalloc((void**)&d_a, size);
    cudaMalloc((void**)&d_b, size);
    cudaMalloc((void**)&d_c, size);

    a = (int *)malloc(size); random_ints(a,N);
    b = (int *)malloc(size); random_ints(b,N);
    c = (int *)malloc(size);

    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);

    add<<<N,1>>>(d_a, d_b, d_c);

    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);

    free(a); free(b); free(c);
    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);

    return 0;
}
```

### Vector Addition - Thread

